{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import dotenv\n",
    "from crewai import Crew, Agent, Task, tools\n",
    "from langchain_community.chat_models import ChatLiteLLM\n",
    "import litellm\n",
    "# import XDATA\n",
    "from XDATA import create_x_data_collector_task\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class XSentimentAnalysisProject:\n",
    "    def __init__(self, model_provider='openai'):\n",
    "        \"\"\"\n",
    "        Initialize the X Sentiment Analysis Project with configurable LLM.\n",
    "        \n",
    "        Args:\n",
    "            model_provider (str): LLM provider (openai, anthropic, etc.)\n",
    "        \"\"\"\n",
    "        # Configure LLM with litellm for flexibility\n",
    "        self.llm = ChatLiteLLM(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    def create_x_data_collector_agent(self):\n",
    "        \"\"\"\n",
    "        Create an agent responsible for collecting X (Twitter) data.\n",
    "    \n",
    "        Returns:\n",
    "            Agent: X Data Collector Agent\n",
    "        \"\"\"\n",
    "        return Agent(\n",
    "            role='X Data Collector',\n",
    "            goal='Retrieve comprehensive tweet data from target X creators',\n",
    "            backstory=\"\"\"You are a sophisticated data gathering expert specialized \n",
    "            in social media content extraction. Your mission is to collect tweets, \n",
    "            analyze their context, and prepare structured data for further analysis.\"\"\",\n",
    "            tools=[\n",
    "                # Replace web scraping tools with our custom XDataScraper\n",
    "                    tools.PythonTool(\n",
    "                    description=\"Use XDataScraper to collect tweets from specified X creators\",\n",
    "                    # Point to the create_x_data_collector_task function from the scraper\n",
    "                    python_function=\"create_x_data_collector_task\"\n",
    "                )\n",
    "            ],\n",
    "            verbose=True,\n",
    "            allow_delegation=True\n",
    "        )\n",
    "\n",
    "    def create_sentiment_analyzer_agent(self):\n",
    "        \"\"\"\n",
    "        Create an agent for sentiment and contextual analysis of collected tweets.\n",
    "        \n",
    "        Returns:\n",
    "            Agent: Sentiment Analyzer Agent\n",
    "        \"\"\"\n",
    "        return Agent(\n",
    "            role='Social Media Sentiment Analyst',\n",
    "            goal='Perform deep sentiment and thematic analysis on collected tweet data',\n",
    "            backstory=\"\"\"You are an expert in natural language processing and \n",
    "            social media content interpretation. Your advanced analytical skills \n",
    "            help extract nuanced insights from digital communication.\"\"\",\n",
    "            tools=[\n",
    "                tools.RAGTool(),  # CrewAI's RAG tool for contextual analysis\n",
    "            ],\n",
    "            verbose=True,\n",
    "            allow_delegation=True\n",
    "        )\n",
    "\n",
    "    def create_data_collection_task(self, data_collector_agent, creators):\n",
    "        \"\"\"\n",
    "        Design a task for collecting tweet data from specified creators.\n",
    "    \n",
    "        Args:\n",
    "            data_collector_agent (Agent): X Data Collector Agent\n",
    "            creators (list): List of X creator usernames\n",
    "    \n",
    "        Returns:\n",
    "            Task: Data collection task\n",
    "        \"\"\"\n",
    "        return Task(\n",
    "            description=f\"\"\"Collect tweets from the following X creators: {', '.join(creators)}\n",
    "            - Use XDataScraper to scrape tweets\n",
    "            - Collect comprehensive tweet metadata including:\n",
    "              * Tweet content\n",
    "              * Timestamp\n",
    "              * Engagement metrics (likes, retweets, quotes)\n",
    "              * User information\n",
    "            - Ensure data is stored in a structured JSON format\n",
    "            - Respect data privacy and platform terms of service\"\"\",\n",
    "            agent=data_collector_agent,\n",
    "            tools=[\n",
    "                tools.PythonTool(\n",
    "                    description=\"Execute tweet data collection\",\n",
    "                    python_function=\"create_x_data_collector_task\"\n",
    "                )\n",
    "            ],\n",
    "            expected_output=\"\"\"Comprehensive JSON structure containing:\n",
    "            {\n",
    "                \"data_source\": \"X (Twitter)\",\n",
    "                \"users_analyzed\": [\"username1\", \"username2\"],\n",
    "                \"total_tweets_collected\": 100,\n",
    "                \"tweets\": [\n",
    "                    {\n",
    "                        \"username\": \"creator_username\",\n",
    "                        \"content\": \"Tweet text content\",\n",
    "                        \"created_at\": \"ISO 8601 timestamp\",\n",
    "                        \"likes\": 42,\n",
    "                        \"retweets\": 10,\n",
    "                        \"quote_count\": 5,\n",
    "                        \"reply_count\": 3,\n",
    "                        \"language\": \"en\"\n",
    "                    }\n",
    "                ]\n",
    "            }\"\"\"\n",
    "        )\n",
    "\n",
    "    def create_sentiment_analysis_task(self, sentiment_analyzer_agent, collected_data):\n",
    "        \"\"\"\n",
    "        Design a task for sentiment and thematic analysis.\n",
    "        \n",
    "        Args:\n",
    "            sentiment_analyzer_agent (Agent): Sentiment Analyzer Agent\n",
    "            collected_data (dict): JSON data of collected tweets\n",
    "        \n",
    "        Returns:\n",
    "            Task: Sentiment analysis task\n",
    "        \"\"\"\n",
    "        return Task(\n",
    "            description=\"\"\"Perform comprehensive sentiment and thematic analysis on collected tweets\n",
    "            - Analyze sentiment for each tweet (positive, negative, neutral)\n",
    "            - Identify dominant themes and topics across tweets\n",
    "            - Generate a detailed PDF report with insights\n",
    "            - Include visualizations of sentiment distribution\n",
    "            - Provide actionable insights about each creator's content\"\"\",\n",
    "            agent=sentiment_analyzer_agent,\n",
    "            context=[collected_data],\n",
    "            expected_output=\"\"\"PDF Report with:\n",
    "            1. Overall sentiment analysis\n",
    "            2. Creator-specific insights\n",
    "            3. Thematic breakdown\n",
    "            4. Engagement metrics visualization\"\"\"\n",
    "        )\n",
    "\n",
    "    def run_analysis(self, creators):\n",
    "        \"\"\"\n",
    "        Execute the complete X sentiment analysis workflow.\n",
    "        \n",
    "        Args:\n",
    "            creators (list): List of X creator usernames\n",
    "        \"\"\"\n",
    "        collected_data = create_x_data_collector_task(creators)\n",
    "\n",
    "        # Initialize agents\n",
    "        data_collector = self.create_x_data_collector_agent()\n",
    "        sentiment_analyzer = self.create_sentiment_analyzer_agent()\n",
    "\n",
    "        # Create sentiment analysis task with the collected data\n",
    "        sentiment_analysis_task = self.create_sentiment_analysis_task(\n",
    "            sentiment_analyzer, \n",
    "            collected_data  # Pass the actual collected data instead of an empty placeholder\n",
    "        )\n",
    "\n",
    "        # Create crew and execute workflow\n",
    "        crew = Crew(\n",
    "            agents=[sentiment_analyzer],  # Remove data collector since scraping is now pre-done\n",
    "            tasks=[sentiment_analysis_task],\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        # Run the workflow\n",
    "        result = crew.kickoff()\n",
    "        return result\n",
    "\n",
    "def main():\n",
    "    # Example X creators to analyze\n",
    "    x_creators = [\n",
    "        'elonmusk', 'naval', 'paulg', \n",
    "        'sama', 'lex_fridman', 'balajis'\n",
    "    ]\n",
    "\n",
    "    # Initialize and run project\n",
    "    project = XSentimentAnalysisProject()\n",
    "    analysis_results = project.run_analysis(x_creators)\n",
    "    \n",
    "    # Optional: Save or further process results\n",
    "    print(\"Analysis Complete:\", analysis_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'crewai.tools' has no attribute 'PythonTool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 171\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Initialize and run project\u001b[39;00m\n\u001b[0;32m    170\u001b[0m project \u001b[38;5;241m=\u001b[39m XSentimentAnalysisProject()\n\u001b[1;32m--> 171\u001b[0m analysis_results \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_creators\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Optional: Save or further process results\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis Complete:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analysis_results)\n",
      "Cell \u001b[1;32mIn[2], line 142\u001b[0m, in \u001b[0;36mXSentimentAnalysisProject.run_analysis\u001b[1;34m(self, creators)\u001b[0m\n\u001b[0;32m    139\u001b[0m collected_data \u001b[38;5;241m=\u001b[39m create_x_data_collector_task(creators)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Initialize agents\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m data_collector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_x_data_collector_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m sentiment_analyzer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_sentiment_analyzer_agent()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Create sentiment analysis task with the collected data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m, in \u001b[0;36mXSentimentAnalysisProject.create_x_data_collector_agent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_x_data_collector_agent\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    Create an agent responsible for collecting X (Twitter) data.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m        Agent: X Data Collector Agent\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Agent(\n\u001b[0;32m     20\u001b[0m         role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX Data Collector\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m         goal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieve comprehensive tweet data from target X creators\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     22\u001b[0m         backstory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a sophisticated data gathering expert specialized \u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m        in social media content extraction. Your mission is to collect tweets, \u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124m        analyze their context, and prepare structured data for further analysis.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[0;32m     25\u001b[0m         tools\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;66;03m# Replace web scraping tools with our custom XDataScraper\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m                 \u001b[43mtools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonTool\u001b[49m(\n\u001b[0;32m     28\u001b[0m                 description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse XDataScraper to collect tweets from specified X creators\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m                 \u001b[38;5;66;03m# Point to the create_x_data_collector_task function from the scraper\u001b[39;00m\n\u001b[0;32m     30\u001b[0m                 python_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_x_data_collector_task\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m             )\n\u001b[0;32m     32\u001b[0m         ],\n\u001b[0;32m     33\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     34\u001b[0m         allow_delegation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'crewai.tools' has no attribute 'PythonTool'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-23 22:26:35.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 22:41:35\u001b[0m\n",
      "\u001b[32m2025-01-23 22:26:35.542\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 22:41:35\u001b[0m\n",
      "\u001b[32m2025-01-23 22:26:35.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 22:41:35\u001b[0m\n",
      "\u001b[32m2025-01-23 22:26:35.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 22:41:35\u001b[0m\n",
      "\u001b[32m2025-01-23 22:26:35.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mNo account available for queue \"SearchTimeline\". Next available at 22:41:35\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping tweets for elonmusk: 'Tweet' object has no attribute 'createdAt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-23 22:26:40.550\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account hellmate9080 on queue SearchTimeline\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping tweets for naval: 'Tweet' object has no attribute 'createdAt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-23 22:26:45.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account hellmate9080 on queue SearchTimeline\u001b[0m\n",
      "\u001b[32m2025-01-23 22:26:51.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account hellmate9080 on queue SearchTimeline\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping tweets for sama: 'Tweet' object has no attribute 'createdAt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-23 22:26:56.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account hellmate9080 on queue SearchTimeline\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping tweets for paulg: 'Tweet' object has no attribute 'createdAt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-23 22:27:02.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtwscrape.accounts_pool\u001b[0m:\u001b[36mget_for_queue_or_wait\u001b[0m:\u001b[36m308\u001b[0m - \u001b[1mContinuing with account hellmate9080 on queue SearchTimeline\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error scraping tweets for balajis: 'Tweet' object has no attribute 'createdAt'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
